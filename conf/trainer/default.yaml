_target_: pytorch_lightning.Trainer
accelerator: 'gpu'
devices: [1,2]
num_nodes: 1
precision: 32
max_epochs: ${data_module.max_epochs}
accumulate_grad_batches: 1
gradient_clip_val: 0.5
gradient_clip_algorithm: value
check_val_every_n_epoch: 5
enable_checkpointing: True

strategy:
  _target_: pytorch_lightning.strategies.ddp.DDPStrategy
  find_unused_parameters: true
  
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: 'DDPM'
  name: 'alex_mp_20_xrd'
  # job_type: train
  offline: False
  settings:
    _target_: wandb.Settings
    start_method: fork
    _save_requirements: False

callbacks:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: None
  filename: "best-{epoch:02d}-{loss_val:.2f}"
  monitor: "loss_val"
  mode: "min"
  save_top_k: 1
  save_last: True